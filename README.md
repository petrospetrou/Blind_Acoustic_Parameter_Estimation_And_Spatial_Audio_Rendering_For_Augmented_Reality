# Blind Acoustic Parameter Estimation and Spatial Audio Rendering for Augmented Reality

This repository contains the code, data, and results for my MSc thesis project at **Queen Mary University of London**:  
**Blind Acoustic Parameter Estimation and Spatial Audio Rendering Using Deep Learning for Augmented Reality**  
(Petros Petrou, 2025, supervised by Dr. Aidan Hogg).

The project presents a complete end-to-end pipeline for estimating acoustic parameters from speech and using them to render immersive spatial audio. The goal is to enable **scene-aware, realistic audio rendering for augmented reality applications** without requiring manual room measurements or specialized hardware.

---

## Abstract

This thesis presents a complete end-to-end system for blind acoustic parameter estimation and spatial audio rendering in augmented reality (AR) environments.  
A deep learning model based on a **Convolutional Recurrent Neural Network (CRNN)** is trained on synthetically generated reverberant speech to predict key acoustic parameters — **RT60, Direct-to-Reverberant Ratio (DRR), and Speech Clarity Index (C50)** — directly from mono audio.  

The predicted parameters are then used to drive a **parametric rendering module** based on **Feedback Delay Networks (FDNs)**, producing binaural audio consistent with the estimated room acoustics.  

Evaluation using both objective metrics (MAE, Pearson’s r, PESQ, STOI) and subjective listening confirms that the system reliably estimates room characteristics and synthesizes perceptually realistic, intelligible spatial audio.  
The results highlight the feasibility of blind acoustic modeling as a scalable front-end for immersive AR audio applications.

---

## Directory Structure

```
Blind_Acoustic_Parameter_Estimation_And_Spatial_Audio_Rendering_For_Augmented_Reality/
│
├── data/                        # Synthetic training dataset (included) and PDF with link to real dataset
├── dry_speech/                  # Clean speech recordings used for data generation
├── features/                    # Extracted features (e.g., log-Mel spectrograms)
├── labels/                      # Target labels for training (RT60, DRR, C50)
├── models/                      # Model definitions and saved weights
│   └── crnn_dropout_bigru_v1.pt # Main trained model used for inference and Phase II
├── notebooks/                   # Jupyter notebooks for training, inference, and evaluation
│   ├── Phase I/                 # Blind acoustic parameter estimation
│   └── Phase II/                # Rendering and evaluation pipeline
├── Evaluation/                  # Evaluation results and scripts
├── Output/                      # Experimental outputs (figures, generated audio, logs)
├── Additional Requirements/     # Supplementary materials
├── Blind_Acoustic_Parameter_Estimation_And_Spatial_Audio_Rendering_Using_Deep_Learning_for_Augmented_Reality.pdf
│                                # Full MSc thesis
└── README.md                    # Project documentation (this file)
```

---

## Usage

No environment setup or external dependencies are required — everything is contained in the Jupyter notebooks.

1. Clone the repository:
   ```bash
   git clone https://github.com/petrospetrou/Blind_Acoustic_Parameter_Estimation_And_Spatial_Audio_Rendering_For_Augmented_Reality.git
   cd Blind_Acoustic_Parameter_Estimation_And_Spatial_Audio_Rendering_For_Augmented_Reality
   ```

2. Open the notebooks in the `notebooks/` folder:  
   - **Phase I** notebooks: Train and test the CRNN model for blind acoustic parameter estimation.  
   - **Phase II** notebooks: Perform spatial audio rendering and evaluation.  

3. Adjust **file paths** inside the notebooks as needed for your local machine.

---

## Data

- **Synthetic dataset (Phase I)**: Included in the repository.  
  Generated by convolving clean speech from LibriSpeech with simulated Room Impulse Responses (RIRs).  

- **Real-world dataset (Phase II)**: Not included due to size.  
  A **PDF inside the `data/` folder** contains the download link.  

- Paths must be updated inside the notebooks to match your local dataset locations.

---

## Models

- Model architectures and training functions are included in the repo.  
- Pre-trained weights are stored in the `models/` folder.  
- The file **`crnn_dropout_bigru_v1.pt`** is the trained CRNN checkpoint used for inference and Phase II experiments.

---

## Evaluation

- **Metrics**: MAE, RMSE, Pearson’s correlation coefficient (r), R², PESQ, STOI.  
- **Results**:  
  - Implementations in `notebooks/Phase II/Phase_II_Evaluation/`.  
  - Additional results in the `Evaluation/` folder.  
- Full analysis is presented in the thesis PDF.  

Key findings:
- CRNN predictions: high correlation with ground truth (e.g., DRR r = 0.996, C50 r = 0.964).  
- Rendering results: **PESQ ≈ 3.0**, **STOI ≈ 0.96**, confirming perceptually realistic and intelligible audio.  
- Subjective evaluation: Informal listening confirmed plausibility and realism of the rendered spatial audio.

---

## Results

- Phase I: CRNN estimator achieved strong prediction accuracy on synthetic reverberant speech.  
- Phase II: Predicted parameters successfully drove an FDN-based rendering pipeline, producing binaural spatial audio consistent with estimated room acoustics.  
- Evaluation confirmed both **objective accuracy** and **perceptual plausibility** of the generated audio.

---

## Citation

If you use this work in academic research, please cite:

```
@mastersthesis{petrou2025blind,
  title     = {Blind Acoustic Parameter Estimation and Spatial Audio Rendering Using Deep Learning for Augmented Reality},
  author    = {Petros Petrou},
  school    = {Queen Mary University of London},
  year      = {2025}
}
```

---

## Acknowledgements

This repository is part of my **MSc Computer Science dissertation** at **Queen Mary University of London**, under the supervision of **Dr. Aidan Hogg**.

---

## License

This project is part of an academic MSc thesis and is **not intended for reuse or redistribution**.
